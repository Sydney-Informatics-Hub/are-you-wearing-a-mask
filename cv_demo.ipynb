{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cv-demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVJ68vRsBCP/krvgCjtPB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlydecker/are-you-wearing-a-mask/blob/main/cv_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision Demo\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook shows how you can use a Google Colab VM for training YOLO object detection models using data from Kaggle.\n",
        "\n",
        "**Links**\n",
        "- [Face Mask Detection](https://github.com/hlydecker/are-you-wearing-a-mask) GitHub repository.\n",
        "- [Face Masks Dataset](https://www.kaggle.com/datasets/henrylydecker/face-masks) on Kaggle.\n",
        "- [Prediction Web App](https://huggingface.co/spaces/hlydecker/are-you-wearing-a-mask) on Huggingface Spaces."
      ],
      "metadata": {
        "id": "nTXnvsYhGPU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Before we get started, we need to set a few things up. To train our model, we will be using a Google Colab notebook. If you aren't familiar with Colab, it is a service provided by Google where users can run notebooks on Google Cloud Platform virtual machines. At the moment, they provide access to GPUs for users on the free tier. We will be using a Google Colab GPU machine for this demonstation.\n",
        "\n",
        "## Step 1: Install core dependencies\n",
        "\n",
        "We need to clone the YOLOv5 GitHub repository, the mask detection GitHub repository, and install kaggle. \n",
        "We also need to make a .kaggle directory."
      ],
      "metadata": {
        "id": "ybtMXUcGGZ2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5 \n",
        "!git clone https://github.com/hlydecker/are-you-wearing-a-mask\n",
        "\n",
        "%pip install kaggle\n",
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "8pYpfSkWGhSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Download data from Kaggle\n",
        "\n",
        "To intereact with Kaggle datasets, you need to have a user account. \n",
        "Once you've created a user account, download your `kaggle.json` API key. \n",
        "You'll then need to upload your API key into the Colab's content directory (where we are working right now).\n",
        "\n",
        "After the upload has finished, we will set things up so that Kaggle knows where our API key is.\n",
        "Then we can simply download and unzip our dataset."
      ],
      "metadata": {
        "id": "mxijPEPzGelQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hiblMh5LgpbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d henrylydecker/face-masks\n",
        "!unzip face-masks.zip"
      ],
      "metadata": {
        "id": "qmafijl_gqFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Install YOLOv5\n",
        "\n",
        "We are now ready to install YOLOv5. \n",
        "\n",
        "If this all works correctly, we can then print out a message which will let us know if everything is set up right. \n",
        "Lookout to make sure that this message has \"CUDA\" in it."
      ],
      "metadata": {
        "id": "2Iqlt9g6h8HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow wandb\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images"
      ],
      "metadata": {
        "id": "9qYCj7KfGiP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "metadata": {
        "id": "N9RPQIm4GnEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training\n",
        "\n",
        "Before we can train, we also want to make sure that we have our dataset's .yaml file set correctly, and that everything is in the correct place. \n",
        "You might need to move files around a bit!\n",
        "HINT: Colab has a super useful feature allowing you to copy paths to items using the file explorer.\n",
        "\n",
        "To train our YOLO model, we need to run the `train.py` script, with a few parameters set.\n",
        "\n",
        "These parameters are:\n",
        "- `img`: this tells the model what size of image to work with. YOLOv5 is built for 640p images, so we will set this to 640.\n",
        "- `batch`: this defines the batch size for training. This should be set to the largest batch size allowed by our GPU's VRAM. I've set this to 16 for our Google Colab GPU VM.\n",
        "- `epochs`: this tells the model how many training/evaluation rounds to run. Ideally we should set this value to at least 300. The model will stop training if things don't improve for 100 epochs, but generally 300 is a great starting point.\n",
        "- `data`: this points YOLO to our dataset's .yaml file. This .yaml file includes info about where the data is, and what classes are in the dataset.\n",
        "- `project`: a name for our project. Defaults to something generic, so feel free to pick a more informative name!\n",
        "- `weights`: defines the pre trained starting point for our model to fine tune from. I've used YOLOv5s because it is accurate enough while being lean enough to run on limited compute hardware. Fancier, bigger, better models are more complicated and need more compute.\n",
        "\n",
        "Tip: `train.py` has many more arguments! Check them out to learn more about all of the ways you can control and modify your model training."
      ],
      "metadata": {
        "id": "fEMAwGYTiVh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "qvZpDqH9GqxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python yolov5/train.py --img 640 --batch 16 --epochs 100 --data data.yaml --project cv-demo --weights yolov5s.pt"
      ],
      "metadata": {
        "id": "h2TOxVGVGsjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Prediction\n",
        "\n",
        "Once we have a trained model, we can experiment with predictions!\n",
        "\n",
        "Tell the model what to predict on using the `--source` parameter. \n",
        "\n",
        "- If you put a directory here, the model will predict on any images inside the directory.\n",
        "- If you paste a youtube link the model will predict on the youtube video.\n",
        "- 0 for webcam prediction. You need to do fancy stuff with javascript to make this work on Colab.\n",
        "\n",
        "Predictions are saved in the `yolov5/runs/detect` directory."
      ],
      "metadata": {
        "id": "o3NuYf8ujg-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python yolov5/detect.py --source \"https://www.youtube.com/watch?v=ayy6Wb8zcyc\" --weights \"/content/cv-demo/exp/weights/best.pt\"  "
      ],
      "metadata": {
        "id": "TDMjt3gLG0zs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}